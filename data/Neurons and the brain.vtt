WEBVTT

1
00:00:00.000 --> 00:00:01.980
When neural networks were first

2
00:00:01.980 --> 00:00:03.795
invented many decades ago,

3
00:00:03.795 --> 00:00:05.580
the original motivation was to

4
00:00:05.580 --> 00:00:07.320
write software that could mimic

5
00:00:07.320 --> 00:00:08.850
how the human brain or how

6
00:00:08.850 --> 00:00:11.655
the biological brain
learns and thinks.

7
00:00:11.655 --> 00:00:14.310
Even though today,
neural networks,

8
00:00:14.310 --> 00:00:17.220
sometimes also called
artificial neural networks,

9
00:00:17.220 --> 00:00:20.100
have become very
different than how any of

10
00:00:20.100 --> 00:00:21.150
us might think about how

11
00:00:21.150 --> 00:00:23.085
the brain actually
works and learns.

12
00:00:23.085 --> 00:00:25.169
Some of the biological
motivations

13
00:00:25.169 --> 00:00:26.730
still remain in the way we think

14
00:00:26.730 --> 00:00:28.980
about artificial neural networks

15
00:00:28.980 --> 00:00:30.975
or computer neural
networks today.

16
00:00:30.975 --> 00:00:33.780
Let's start by taking a
look at how the brain

17
00:00:33.780 --> 00:00:36.805
works and how that relates
to neural networks.

18
00:00:36.805 --> 00:00:39.980
The human brain, or
maybe more generally,

19
00:00:39.980 --> 00:00:43.670
the biological brain demonstrates
a higher level or more

20
00:00:43.670 --> 00:00:45.515
capable level of
intelligence and

21
00:00:45.515 --> 00:00:48.080
anything else would be
on the bill so far.

22
00:00:48.080 --> 00:00:50.885
So neural networks
has started with

23
00:00:50.885 --> 00:00:52.850
the motivation of
trying to build

24
00:00:52.850 --> 00:00:55.480
software to mimic the brain.

25
00:00:55.480 --> 00:00:59.585
Work in neural networks had
started back in the 1950s,

26
00:00:59.585 --> 00:01:02.290
and then it fell out
of favor for a while.

27
00:01:02.290 --> 00:01:05.660
Then in the 1980s
and early 1990s,

28
00:01:05.660 --> 00:01:08.840
they gained in popularity
again and showed

29
00:01:08.840 --> 00:01:11.210
tremendous traction
in some applications

30
00:01:11.210 --> 00:01:13.640
like handwritten
digit recognition,

31
00:01:13.640 --> 00:01:15.560
which were used
even backed then to

32
00:01:15.560 --> 00:01:17.780
read postal codes for writing

33
00:01:17.780 --> 00:01:19.235
mail and for reading

34
00:01:19.235 --> 00:01:22.720
dollar figures in
handwritten checks.

35
00:01:22.720 --> 00:01:26.750
But then it fell out of favor
again in the late 1990s.

36
00:01:26.750 --> 00:01:30.560
It was from about
2005 that it enjoyed

37
00:01:30.560 --> 00:01:33.320
a resurgence and also became

38
00:01:33.320 --> 00:01:36.970
re-branded little bit
with deep learning.

39
00:01:36.970 --> 00:01:39.725
One of the things that
surprised me back then

40
00:01:39.725 --> 00:01:41.180
was deep learning and

41
00:01:41.180 --> 00:01:43.735
neural networks meant
very similar things.

42
00:01:43.735 --> 00:01:45.905
But maybe under-appreciated

43
00:01:45.905 --> 00:01:48.110
at the time that the
term deep learning,

44
00:01:48.110 --> 00:01:49.340
just sounds much better

45
00:01:49.340 --> 00:01:51.710
because it's deep
and this learning.

46
00:01:51.710 --> 00:01:54.110
So that turned out
to be the brand that

47
00:01:54.110 --> 00:01:57.190
took off in the last decade
or decade and a half.

48
00:01:57.190 --> 00:01:59.120
Since then, neural networks have

49
00:01:59.120 --> 00:02:02.690
revolutionized application
area after application area.

50
00:02:02.690 --> 00:02:05.270
I think the first
application area

51
00:02:05.270 --> 00:02:07.280
that modern neural
networks or deep learning,

52
00:02:07.280 --> 00:02:10.570
had a huge impact on was
probably speech recognition,

53
00:02:10.570 --> 00:02:12.200
where we started to see

54
00:02:12.200 --> 00:02:14.705
much better speech
recognition systems due to

55
00:02:14.705 --> 00:02:17.060
modern deep learning
and authors such as

56
00:02:17.060 --> 00:02:20.195
[inaudible] and Geoff Hinton
were instrumental to this,

57
00:02:20.195 --> 00:02:24.775
and then it started to make
inroads into computer vision.

58
00:02:24.775 --> 00:02:26.960
Sometimes people still speak of

59
00:02:26.960 --> 00:02:29.860
the ImageNet moments in 2012,

60
00:02:29.860 --> 00:02:33.620
and that was maybe a bigger
splash where then [inaudible]

61
00:02:33.620 --> 00:02:35.510
draw their imagination and

62
00:02:35.510 --> 00:02:37.880
had a big impact on
computer vision.

63
00:02:37.880 --> 00:02:39.455
Then the next few years,

64
00:02:39.455 --> 00:02:40.940
it made us inroads into

65
00:02:40.940 --> 00:02:43.400
texts or into natural
language processing,

66
00:02:43.400 --> 00:02:44.910
and so on and so forth.

67
00:02:44.910 --> 00:02:47.990
Now, neural networks are
used in everything from

68
00:02:47.990 --> 00:02:51.050
climate change to medical
imaging to online advertising

69
00:02:51.050 --> 00:02:53.840
to prouduct recommendations
and really lots of

70
00:02:53.840 --> 00:02:55.730
application areas
of machine learning

71
00:02:55.730 --> 00:02:57.410
now use neural networks.

72
00:02:57.410 --> 00:02:59.915
Even though today's
neural networks have

73
00:02:59.915 --> 00:03:03.400
almost nothing to do with
how the brain learns,

74
00:03:03.400 --> 00:03:06.050
there was the early
motivation of

75
00:03:06.050 --> 00:03:09.020
trying to build software
to mimic the brain.

76
00:03:09.020 --> 00:03:11.525
So how does the brain work?

77
00:03:11.525 --> 00:03:13.640
Here's a diagram illustrating

78
00:03:13.640 --> 00:03:16.400
what neurons in a
brain look like.

79
00:03:16.400 --> 00:03:18.695
All of human thought is from

80
00:03:18.695 --> 00:03:21.485
neurons like this in
your brain and mine,

81
00:03:21.485 --> 00:03:23.600
sending electrical impulses and

82
00:03:23.600 --> 00:03:26.830
sometimes forming new
connections of other neurons.

83
00:03:26.830 --> 00:03:29.120
Given a neuron like this one,

84
00:03:29.120 --> 00:03:32.299
it has a number of
inputs where it receives

85
00:03:32.299 --> 00:03:35.675
electrical impulses
from other neurons,

86
00:03:35.675 --> 00:03:37.640
and then this neuron that I've

87
00:03:37.640 --> 00:03:41.120
circled carries out
some computations and

88
00:03:41.120 --> 00:03:43.040
will then send this outputs

89
00:03:43.040 --> 00:03:46.925
to other neurons by this
electrical impulses,

90
00:03:46.925 --> 00:03:49.730
and this upper neuron's
output in turn

91
00:03:49.730 --> 00:03:53.180
becomes the input to
this neuron down below,

92
00:03:53.180 --> 00:03:55.160
which again aggregates
inputs from

93
00:03:55.160 --> 00:03:58.535
multiple other neurons to then
maybe send its own output,

94
00:03:58.535 --> 00:04:00.140
to yet other neurons,

95
00:04:00.140 --> 00:04:04.435
and this is the stuff of
which human thought is made.

96
00:04:04.435 --> 00:04:09.575
Here's a simplified diagram
of a biological neuron.

97
00:04:09.575 --> 00:04:14.090
A neuron comprises a cell
body shown here on the left,

98
00:04:14.090 --> 00:04:17.210
and if you have taken
a class in biology,

99
00:04:17.210 --> 00:04:22.070
you may recognize this to be
the nucleus of the neuron.

100
00:04:22.070 --> 00:04:24.605
As we saw on the previous slide,

101
00:04:24.605 --> 00:04:26.750
the neuron has different inputs.

102
00:04:26.750 --> 00:04:28.880
In a biological neuron,

103
00:04:28.880 --> 00:04:32.770
the input wires are
called the dendrites,

104
00:04:32.770 --> 00:04:35.810
and it then occasionally
sends electrical impulses

105
00:04:35.810 --> 00:04:38.735
to other neurons via
the output wire,

106
00:04:38.735 --> 00:04:40.465
which is called the axon.

107
00:04:40.465 --> 00:04:42.725
Don't worry about these
biological terms.

108
00:04:42.725 --> 00:04:44.900
If you saw them in
a biology class,

109
00:04:44.900 --> 00:04:45.950
you may remember them,

110
00:04:45.950 --> 00:04:47.240
but you don't really need to

111
00:04:47.240 --> 00:04:48.710
memorize any of these terms for

112
00:04:48.710 --> 00:04:52.115
the purpose of building
artificial neural networks.

113
00:04:52.115 --> 00:04:54.860
But this biological
neuron may then send

114
00:04:54.860 --> 00:04:56.905
electrical impulses that become

115
00:04:56.905 --> 00:04:59.925
the input to another neuron.

116
00:04:59.925 --> 00:05:03.515
So the artificial
neural network uses

117
00:05:03.515 --> 00:05:06.290
a very simplified
Mathematical model

118
00:05:06.290 --> 00:05:09.820
of what a biological
neuron does.

119
00:05:09.820 --> 00:05:12.800
I'm going to draw
a little circle

120
00:05:12.800 --> 00:05:16.490
here to denote a single neuron.

121
00:05:16.490 --> 00:05:20.045
What a neuron does is
it takes some inputs,

122
00:05:20.045 --> 00:05:21.470
one or more inputs,

123
00:05:21.470 --> 00:05:23.720
which are just numbers.

124
00:05:23.720 --> 00:05:26.090
It does some computation

125
00:05:26.090 --> 00:05:28.445
and it outputs
some other number,

126
00:05:28.445 --> 00:05:32.315
which then could be an
input to a second neuron,

127
00:05:32.315 --> 00:05:34.265
shown here on the right.

128
00:05:34.265 --> 00:05:36.515
When you're building an
artificial neural network

129
00:05:36.515 --> 00:05:38.285
or deep learning algorithm,

130
00:05:38.285 --> 00:05:41.345
rather than building
one neuron at a time,

131
00:05:41.345 --> 00:05:42.740
you often want to

132
00:05:42.740 --> 00:05:46.910
simulate many such
neurons at the same time.

133
00:05:46.910 --> 00:05:52.790
In this diagram, I'm
drawing three neurons.

134
00:05:52.790 --> 00:05:54.605
What these neurons do

135
00:05:54.605 --> 00:05:57.425
collectively is
input a few numbers,

136
00:05:57.425 --> 00:05:59.345
carry out some computation,

137
00:05:59.345 --> 00:06:02.090
and output some other numbers.

138
00:06:02.090 --> 00:06:03.410
Now, at this point,

139
00:06:03.410 --> 00:06:05.870
I'd like to give one big caveat,

140
00:06:05.870 --> 00:06:07.565
which is that even though I made

141
00:06:07.565 --> 00:06:09.500
a loose analogy between

142
00:06:09.500 --> 00:06:12.125
biological neurons and
artificial neurons,

143
00:06:12.125 --> 00:06:13.595
I think that today we have

144
00:06:13.595 --> 00:06:16.505
almost no idea how the
human brain works.

145
00:06:16.505 --> 00:06:18.125
In fact, every few years,

146
00:06:18.125 --> 00:06:20.900
neuroscientists make some
fundamental breakthrough

147
00:06:20.900 --> 00:06:22.490
about how the brain works.

148
00:06:22.490 --> 00:06:23.720
I think we'll continue to do

149
00:06:23.720 --> 00:06:25.910
so for the foreseeable future.

150
00:06:25.910 --> 00:06:28.310
That to me is a
sign that there are

151
00:06:28.310 --> 00:06:30.170
many breakthroughs
that are yet to be

152
00:06:30.170 --> 00:06:32.840
discovered about how the
brain actually works,

153
00:06:32.840 --> 00:06:34.670
and thus attempts to blindly

154
00:06:34.670 --> 00:06:37.310
mimic what we know of
the human brain today,

155
00:06:37.310 --> 00:06:39.395
which is frankly very little,

156
00:06:39.395 --> 00:06:41.240
probably won't get us that far

157
00:06:41.240 --> 00:06:43.130
toward building
raw intelligence.

158
00:06:43.130 --> 00:06:44.990
Certainly not with
our current level

159
00:06:44.990 --> 00:06:46.895
of knowledge in neuroscience.

160
00:06:46.895 --> 00:06:49.460
Having said that, even with

161
00:06:49.460 --> 00:06:52.190
these extremely simplified
models of a neuron,

162
00:06:52.190 --> 00:06:54.560
which we'll talk about,
we'll be able to build

163
00:06:54.560 --> 00:06:57.530
really powerful deep
learning algorithms.

164
00:06:57.530 --> 00:06:59.210
So as you go deeper into

165
00:06:59.210 --> 00:07:01.655
neural networks and
into deep learning,

166
00:07:01.655 --> 00:07:04.925
even though the origins were
biologically motivated,

167
00:07:04.925 --> 00:07:08.105
don't take the biological
motivation too seriously.

168
00:07:08.105 --> 00:07:09.980
In fact, those of us that do

169
00:07:09.980 --> 00:07:11.600
research in deep learning have

170
00:07:11.600 --> 00:07:13.145
shifted away from looking to

171
00:07:13.145 --> 00:07:15.185
biological motivation that much.

172
00:07:15.185 --> 00:07:17.300
But instead, they're just using

173
00:07:17.300 --> 00:07:19.280
engineering principles to figure

174
00:07:19.280 --> 00:07:21.410
out how to build algorithms
that are more effective.

175
00:07:21.410 --> 00:07:24.440
But I think it might still
be fun to speculate and

176
00:07:24.440 --> 00:07:26.330
think about how
biological neurons

177
00:07:26.330 --> 00:07:27.980
work every now and then.

178
00:07:27.980 --> 00:07:29.930
The ideas of neural
networks have

179
00:07:29.930 --> 00:07:31.865
been around for many decades.

180
00:07:31.865 --> 00:07:33.140
A few people have asked me,

181
00:07:33.140 --> 00:07:34.670
"Hey Andrew, why now?

182
00:07:34.670 --> 00:07:37.430
Why is it that only
in the last handful

183
00:07:37.430 --> 00:07:40.670
of years that neural networks
have really taken off?"

184
00:07:40.670 --> 00:07:42.650
This is a picture I draw

185
00:07:42.650 --> 00:07:44.555
for them when I'm
asked that question

186
00:07:44.555 --> 00:07:46.400
and that maybe you could draw

187
00:07:46.400 --> 00:07:48.990
for others as well if they
ask you that question.

188
00:07:48.990 --> 00:07:51.850
Let me plot on the
horizontal axis

189
00:07:51.850 --> 00:07:54.535
the amount of data you
have for a problem,

190
00:07:54.535 --> 00:07:56.230
and on the vertical axis,

191
00:07:56.230 --> 00:07:58.630
the performance or
the accuracy of

192
00:07:58.630 --> 00:08:01.605
a learning algorithm
applied to that problem.

193
00:08:01.605 --> 00:08:04.280
Over the last couple of decades,

194
00:08:04.280 --> 00:08:05.810
with the rise of the Internet,

195
00:08:05.810 --> 00:08:07.175
the rise of mobile phones,

196
00:08:07.175 --> 00:08:09.230
the digitalization
of our society,

197
00:08:09.230 --> 00:08:11.630
the amount of data
we have for a lot of

198
00:08:11.630 --> 00:08:14.870
applications has steadily
marched to the right.

199
00:08:14.870 --> 00:08:17.960
Lot of records that
use P on paper,

200
00:08:17.960 --> 00:08:19.835
such as if you order something

201
00:08:19.835 --> 00:08:22.100
rather than it being
on a piece of paper,

202
00:08:22.100 --> 00:08:24.455
there's much more likely
to be a digital record.

203
00:08:24.455 --> 00:08:26.765
Your health record,
if you see a doctor,

204
00:08:26.765 --> 00:08:28.430
is much more likely
to be digital

205
00:08:28.430 --> 00:08:31.880
now compared to on
pieces of paper.

206
00:08:31.880 --> 00:08:34.880
So in many application areas,

207
00:08:34.880 --> 00:08:37.535
the amount of digital
data has exploded.

208
00:08:37.535 --> 00:08:39.365
What we saw was with

209
00:08:39.365 --> 00:08:41.585
traditional machine-learning
algorithms,

210
00:08:41.585 --> 00:08:45.440
such as logistic regression
and linear regression,

211
00:08:45.440 --> 00:08:48.365
even as you fed those
algorithms more data,

212
00:08:48.365 --> 00:08:50.270
it was very difficult to get

213
00:08:50.270 --> 00:08:52.970
the performance to
keep on going up.

214
00:08:52.970 --> 00:08:54.155
So it was as if

215
00:08:54.155 --> 00:08:56.030
the traditional learning
algorithms like

216
00:08:56.030 --> 00:08:58.250
linear regression and
logistic regression,

217
00:08:58.250 --> 00:09:00.380
they just weren't able to scale

218
00:09:00.380 --> 00:09:02.510
with the amount of data
we could now feed it

219
00:09:02.510 --> 00:09:04.970
and they weren't able to
take effective advantage

220
00:09:04.970 --> 00:09:08.015
of all this data we had for
different applications.

221
00:09:08.015 --> 00:09:12.080
What AI researchers
started to observe was

222
00:09:12.080 --> 00:09:13.955
that if you were to train

223
00:09:13.955 --> 00:09:16.445
a small neural network
on this dataset,

224
00:09:16.445 --> 00:09:19.520
then the performance
maybe looks like this.

225
00:09:19.520 --> 00:09:23.705
If you were to train a
medium-sized neural network,

226
00:09:23.705 --> 00:09:26.360
meaning one with
more neurons in it,

227
00:09:26.360 --> 00:09:28.535
its performance may
look like that.

228
00:09:28.535 --> 00:09:31.415
If you were to train a
very large neural network,

229
00:09:31.415 --> 00:09:34.370
meaning one with a lot of
these artificial neurons,

230
00:09:34.370 --> 00:09:36.080
then for some applications

231
00:09:36.080 --> 00:09:39.020
the performance will
just keep on going up.

232
00:09:39.020 --> 00:09:41.195
So this meant two things,

233
00:09:41.195 --> 00:09:42.830
it meant that for
a certain class of

234
00:09:42.830 --> 00:09:45.365
applications where you
do have a lot of data,

235
00:09:45.365 --> 00:09:48.695
sometimes you hear the
term big data toss around,

236
00:09:48.695 --> 00:09:50.885
if you're able to train

237
00:09:50.885 --> 00:09:53.150
a very large neural
network to take advantage

238
00:09:53.150 --> 00:09:55.625
of that huge amount
of data you have,

239
00:09:55.625 --> 00:09:58.670
then you could attain
performance on anything

240
00:09:58.670 --> 00:10:01.910
ranging from speech recognition,
to image recognition,

241
00:10:01.910 --> 00:10:04.410
to natural language processing
applications and many more,

242
00:10:04.410 --> 00:10:06.190
they just were not possible with

243
00:10:06.190 --> 00:10:09.085
earlier generations of
learning algorithms.

244
00:10:09.085 --> 00:10:12.535
This caused deep learning
algorithms to take off,

245
00:10:12.535 --> 00:10:17.420
and this too is why faster
computer processors,

246
00:10:17.420 --> 00:10:21.845
including the rise of GPUs
or graphics processor units.

247
00:10:21.845 --> 00:10:24.560
This is hardware
originally designed to

248
00:10:24.560 --> 00:10:27.725
generate nice-looking
computer graphics,

249
00:10:27.725 --> 00:10:28.970
but turned out to be really

250
00:10:28.970 --> 00:10:31.970
powerful for deep
learning as well.

251
00:10:31.970 --> 00:10:34.655
That was also a major force in

252
00:10:34.655 --> 00:10:36.590
allowing deep
learning algorithms

253
00:10:36.590 --> 00:10:38.570
to become what it is today.

254
00:10:38.570 --> 00:10:41.345
That's how neural
networks got started,

255
00:10:41.345 --> 00:10:42.980
as well as why they took off

256
00:10:42.980 --> 00:10:45.110
so quickly in the
last several years.

257
00:10:45.110 --> 00:10:46.850
Let's now dive more deeply into

258
00:10:46.850 --> 00:10:49.820
the details of how neural
network actually works.

259
00:10:49.820 --> 00:10:52.530
Please go on to the next video.